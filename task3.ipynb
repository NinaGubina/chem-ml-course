{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** Обучить рекуррентную нейронную сеть на SMILES и сравнить результаты с сетями на фингерпринтах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninag\\mambaforge\\envs\\cc\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_v1.csv', nrows=10000)\n",
    "logP = [MolLogP(Chem.MolFromSmiles(x)) for x in df.SMILES]\n",
    "#logP = pickle.load(open(\"logP\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    \"\"\"\n",
    "    Class to create a vocabulary from a list of SMILES strings and convert them to tokens.\n",
    "    The class keeps the vocabulary in itself and provides methods to convert SMILES strings to tokenized tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize a new Vocabulary object.\n",
    "        \"\"\"\n",
    "        chars = set()\n",
    "        for string in data:\n",
    "            chars.update(string)\n",
    "\n",
    "        self.chars = chars\n",
    "        all_syms = sorted(list(self.chars))\n",
    "        self.data = data\n",
    "        self.c2i = {c: i + 1 for i, c in enumerate(all_syms)}\n",
    "\n",
    "    def tokens(self):\n",
    "        \"\"\"\n",
    "        Convert the SMILES strings in the vocabulary to tokenized tensors.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tokenized tensors, where each tensor corresponds to a SMILES string.\n",
    "        \"\"\"\n",
    "        tensors = [torch.tensor(self.string2ids(string))\n",
    "                   for string in self.data]\n",
    "        return tensors\n",
    "\n",
    "    def string2ids(self, string):\n",
    "        \"\"\"\n",
    "        Convert a single SMILES string to a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            string (str): A SMILES string.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of token IDs, where each ID corresponds to a token in the SMILES string.\n",
    "        \"\"\"\n",
    "        ids = [self.char2id(c) for c in string]\n",
    "        return ids\n",
    "\n",
    "    def char2id(self, char):\n",
    "        \"\"\"\n",
    "        Convert a single character to a token ID.\n",
    "\n",
    "        Args:\n",
    "            char (str): A character in a SMILES string.\n",
    "\n",
    "        Returns:\n",
    "            int: The ID of the corresponding token.\n",
    "        \"\"\"\n",
    "        return self.c2i[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_logP(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset for logP prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, labels, lens):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of input molecules in SMILES format.\n",
    "            labels (list): A list of corresponding logP values.\n",
    "            lens (list): A list of lengths of the input molecules.\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.labels = torch.from_numpy(np.asarray(labels, dtype=float)).type(torch.float)\n",
    "        self.lens = lens\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple of input, label, and length at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple of input, label, and length at the given index.\n",
    "        \"\"\"\n",
    "        return self.inputs[idx], self.labels[idx], self.lens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Vocabulary(df.SMILES)\n",
    "labels = logP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(string) for string in data.tokens()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = Dataset_logP(data.tokens(), labels, lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "test_scaffolds = []\n",
    "\n",
    "for sid, split in enumerate(df.SPLIT):\n",
    "    \"\"\"\n",
    "    Iterate over each row in the 'SPLIT' column of the DataFrame and add the corresponding data point to either \n",
    "    the train, test, or test_scaffolds list based on its value.\n",
    "\n",
    "    Args:\n",
    "        sid (int): The index of the current data point.\n",
    "        split (str): The value of the 'SPLIT' column for the current data point.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if split == 'train':\n",
    "        train.append(data_list[sid])\n",
    "    elif split == 'test':\n",
    "        test.append(data_list[sid])\n",
    "    else:\n",
    "        test_scaffolds.append(data_list[sid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data):\n",
    "    \"\"\"\n",
    "    Collates a batch of data for use in a recurrent neural network (RNN) by padding sequences\n",
    "    to a common length and returning the padded sequences, targets, and original sequence lengths.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of tuples, where each tuple contains a tensor of input sequences,\n",
    "            a tensor of corresponding targets, and an integer representing the length of the\n",
    "            original sequence.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the padded input sequences, targets, and sequence lengths.\n",
    "            The input sequences are padded with zeros to the length of the longest sequence\n",
    "            in the batch, and the targets and sequence lengths are returned as tensors.\n",
    "    \"\"\"\n",
    "    padded_inputs = pad_sequence([t[0] for t in data], batch_first=True)\n",
    "    targets = torch.tensor([t[1] for t in data])\n",
    "    lens = torch.tensor([t[2] for t in data])\n",
    "    return padded_inputs, targets, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN module that performs forward pass of a recurrent neural network.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary including the pad token.\n",
    "\n",
    "    Attributes:\n",
    "        hidden_size (int): The number of features in the hidden state of the LSTM layer.\n",
    "        num_layers (int): Number of recurrent layers. Default is 2.\n",
    "        dropout (float): Dropout probability. Default is 0.3.\n",
    "        vocab_size (int): The size of the vocabulary including the pad token.\n",
    "        embedding_size (int): The size of the input to the LSTM layer.\n",
    "\n",
    "    Methods:\n",
    "        forward(x, lens, hiddens=None): Performs the forward pass of the RNN module.\n",
    "\n",
    "    Returns:\n",
    "        x (tensor): The output of the linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.3\n",
    "        self.embedding_size = 100\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.lstm_layer = nn.LSTM(self.embedding_size, self.hidden_size,\n",
    "                                  self.num_layers, dropout=self.dropout,\n",
    "                                  batch_first=True)\n",
    "        self.linear_layer = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, lens, hiddens=None):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = rnn_utils.pack_padded_sequence(x, lens, batch_first=True, enforce_sorted=False)\n",
    "        x, hiddens = self.lstm_layer(x, hiddens)\n",
    "        x, _ = rnn_utils.pad_packed_sequence(x, batch_first=True)\n",
    "        last_seq_idxs = lens - 1\n",
    "        last_seq_items = x[range(x.shape[0]), last_seq_idxs, :]\n",
    "        x = self.linear_layer(last_seq_items)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "        return (p for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, tqdm_data, criterion, optimizer=None):\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    postfix = {'loss': 0,\n",
    "               'running_loss': 0}\n",
    "    for i, (data, targets, lens) in enumerate(tqdm_data):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(data, lens)\n",
    "        loss = criterion(outputs.flatten(), targets)\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        postfix['loss'] = loss.item()\n",
    "        postfix['running_loss'] += (loss.item() -\n",
    "                                    postfix['running_loss']) / (i + 1)\n",
    "        tqdm_data.set_postfix(postfix)\n",
    "\n",
    "    postfix['mode'] = 'Eval' if optimizer is None else 'Train'\n",
    "    return postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data, criterion):\n",
    "    model.eval()\n",
    "    loss_out = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, targets, lens) in enumerate(data):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(data, lens)\n",
    "            loss = criterion(outputs.flatten(), targets)\n",
    "            loss_out.append(loss)\n",
    "    return sum(loss_out) / len(loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [5, 10, 20]\n",
    "hidden_size = [10, 50, 100]\n",
    "learning_rate = [0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch #0): 100%|██████████| 1660/1660 [00:22<00:00, 74.86it/s, loss=0.376, running_loss=1.06] \n",
      "Training (epoch #1): 100%|██████████| 1660/1660 [00:22<00:00, 74.31it/s, loss=0.341, running_loss=1.02] \n",
      "Training (epoch #2): 100%|██████████| 1660/1660 [00:22<00:00, 75.29it/s, loss=0.337, running_loss=1.01] \n",
      "Training (epoch #3): 100%|██████████| 1660/1660 [00:22<00:00, 73.22it/s, loss=0.342, running_loss=1.01] \n",
      "Training (epoch #4): 100%|██████████| 1660/1660 [00:24<00:00, 66.41it/s, loss=0.345, running_loss=1.01] \n",
      "Training (epoch #0): 100%|██████████| 1660/1660 [00:26<00:00, 62.84it/s, loss=0.205, running_loss=0.598] \n",
      "Training (epoch #1): 100%|██████████| 1660/1660 [00:26<00:00, 62.34it/s, loss=0.202, running_loss=0.44]  \n",
      "Training (epoch #2): 100%|██████████| 1660/1660 [00:26<00:00, 62.86it/s, loss=0.119, running_loss=0.382]  \n",
      "Training (epoch #3): 100%|██████████| 1660/1660 [00:27<00:00, 61.23it/s, loss=0.0572, running_loss=0.38] \n",
      "Training (epoch #4): 100%|██████████| 1660/1660 [00:27<00:00, 60.57it/s, loss=0.107, running_loss=0.357] \n",
      "Training (epoch #0): 100%|██████████| 1660/1660 [00:26<00:00, 62.53it/s, loss=0.225, running_loss=0.807] \n",
      "Training (epoch #1): 100%|██████████| 1660/1660 [00:26<00:00, 62.82it/s, loss=0.104, running_loss=0.303]  \n",
      "Training (epoch #2): 100%|██████████| 1660/1660 [00:27<00:00, 60.15it/s, loss=0.15, running_loss=0.244]  \n",
      "Training (epoch #3): 100%|██████████| 1660/1660 [00:27<00:00, 60.74it/s, loss=0.0466, running_loss=0.216]\n",
      "Training (epoch #4): 100%|██████████| 1660/1660 [00:27<00:00, 59.96it/s, loss=0.153, running_loss=0.197]  \n",
      "Training (epoch #0): 100%|██████████| 1660/1660 [00:27<00:00, 59.86it/s, loss=0.762, running_loss=1.18] \n",
      "Training (epoch #1): 100%|██████████| 1660/1660 [00:27<00:00, 60.64it/s, loss=0.682, running_loss=1.38] \n",
      "Training (epoch #2): 100%|██████████| 1660/1660 [00:27<00:00, 59.95it/s, loss=0.292, running_loss=1.15] \n",
      "Training (epoch #3): 100%|██████████| 1660/1660 [00:26<00:00, 62.55it/s, loss=0.772, running_loss=1.12] \n",
      "Training (epoch #4): 100%|██████████| 1660/1660 [00:27<00:00, 60.21it/s, loss=0.259, running_loss=1.05] \n",
      "Training (epoch #0): 100%|██████████| 1660/1660 [00:26<00:00, 62.09it/s, loss=0.288, running_loss=0.628] \n",
      "Training (epoch #1): 100%|██████████| 1660/1660 [00:26<00:00, 62.91it/s, loss=0.199, running_loss=0.432] \n",
      "Training (epoch #2): 100%|██████████| 1660/1660 [00:24<00:00, 67.05it/s, loss=0.169, running_loss=0.391] \n",
      "Training (epoch #3): 100%|██████████| 1660/1660 [00:24<00:00, 67.07it/s, loss=0.0732, running_loss=0.358]\n",
      "Training (epoch #4): 100%|██████████| 1660/1660 [00:25<00:00, 65.40it/s, loss=0.116, running_loss=0.302]  \n",
      "Training (epoch #0): 100%|██████████| 1660/1660 [00:27<00:00, 60.12it/s, loss=0.249, running_loss=0.429]  \n",
      "Training (epoch #1): 100%|██████████| 1660/1660 [00:27<00:00, 60.36it/s, loss=0.164, running_loss=0.159]  \n",
      "Training (epoch #2): 100%|██████████| 1660/1660 [00:27<00:00, 60.04it/s, loss=0.271, running_loss=0.118]  \n",
      "Training (epoch #3): 100%|██████████| 1660/1660 [00:26<00:00, 61.63it/s, loss=0.181, running_loss=0.0957]  \n",
      "Training (epoch #4): 100%|██████████| 1660/1660 [00:27<00:00, 61.19it/s, loss=0.116, running_loss=0.0799]  \n",
      "Training (epoch #0): 100%|██████████| 1660/1660 [00:26<00:00, 61.51it/s, loss=0.758, running_loss=1.33] \n",
      "Training (epoch #1): 100%|██████████| 1660/1660 [00:27<00:00, 60.49it/s, loss=0.669, running_loss=1.22] \n",
      "Training (epoch #2): 100%|██████████| 1660/1660 [00:27<00:00, 61.11it/s, loss=1.2, running_loss=1.22]    \n",
      "Training (epoch #3): 100%|██████████| 1660/1660 [00:26<00:00, 61.82it/s, loss=0.793, running_loss=1.12] \n",
      "Training (epoch #4): 100%|██████████| 1660/1660 [00:25<00:00, 64.17it/s, loss=0.233, running_loss=1.11] \n",
      "Training (epoch #0): 100%|██████████| 1660/1660 [00:26<00:00, 62.14it/s, loss=0.242, running_loss=0.556] \n",
      "Training (epoch #1): 100%|██████████| 1660/1660 [00:26<00:00, 62.21it/s, loss=0.067, running_loss=0.401] \n",
      "Training (epoch #2): 100%|██████████| 1660/1660 [00:27<00:00, 59.46it/s, loss=0.257, running_loss=0.365] \n",
      "Training (epoch #3): 100%|██████████| 1660/1660 [00:27<00:00, 60.12it/s, loss=0.195, running_loss=0.319]  \n",
      "Training (epoch #4): 100%|██████████| 1660/1660 [00:28<00:00, 57.92it/s, loss=0.0656, running_loss=0.27]  \n",
      "Training (epoch #0): 100%|██████████| 1660/1660 [00:28<00:00, 57.74it/s, loss=0.261, running_loss=0.39]   \n",
      "Training (epoch #1): 100%|██████████| 1660/1660 [00:28<00:00, 57.54it/s, loss=0.091, running_loss=0.142]  \n",
      "Training (epoch #2): 100%|██████████| 1660/1660 [00:28<00:00, 57.47it/s, loss=0.152, running_loss=0.0992] \n",
      "Training (epoch #3): 100%|██████████| 1660/1660 [00:28<00:00, 58.57it/s, loss=0.124, running_loss=0.0795]  \n",
      "Training (epoch #4): 100%|██████████| 1660/1660 [00:28<00:00, 58.56it/s, loss=0.0424, running_loss=0.0625] \n",
      "Training (epoch #0): 100%|██████████| 830/830 [00:15<00:00, 53.43it/s, loss=0.468, running_loss=1.04]\n",
      "Training (epoch #1): 100%|██████████| 830/830 [00:14<00:00, 55.91it/s, loss=0.51, running_loss=1.03] \n",
      "Training (epoch #2): 100%|██████████| 830/830 [00:14<00:00, 57.10it/s, loss=0.513, running_loss=1.03]\n",
      "Training (epoch #3): 100%|██████████| 830/830 [00:14<00:00, 56.81it/s, loss=0.513, running_loss=1.03]\n",
      "Training (epoch #4): 100%|██████████| 830/830 [00:14<00:00, 56.11it/s, loss=0.612, running_loss=1.03]\n",
      "Training (epoch #0): 100%|██████████| 830/830 [00:14<00:00, 56.53it/s, loss=0.173, running_loss=0.817] \n",
      "Training (epoch #1): 100%|██████████| 830/830 [00:14<00:00, 56.36it/s, loss=0.184, running_loss=0.482] \n",
      "Training (epoch #2): 100%|██████████| 830/830 [00:14<00:00, 57.54it/s, loss=0.178, running_loss=0.372] \n",
      "Training (epoch #3): 100%|██████████| 830/830 [00:14<00:00, 56.22it/s, loss=0.23, running_loss=0.302]  \n",
      "Training (epoch #4): 100%|██████████| 830/830 [00:14<00:00, 57.45it/s, loss=0.273, running_loss=0.287] \n",
      "Training (epoch #0): 100%|██████████| 830/830 [00:14<00:00, 55.50it/s, loss=0.408, running_loss=1.14]\n",
      "Training (epoch #1): 100%|██████████| 830/830 [00:14<00:00, 56.36it/s, loss=0.313, running_loss=0.334] \n",
      "Training (epoch #2): 100%|██████████| 830/830 [00:14<00:00, 56.20it/s, loss=0.173, running_loss=0.243] \n",
      "Training (epoch #3): 100%|██████████| 830/830 [00:14<00:00, 56.82it/s, loss=0.137, running_loss=0.203] \n",
      "Training (epoch #4): 100%|██████████| 830/830 [00:14<00:00, 56.44it/s, loss=0.182, running_loss=0.182] \n",
      "Training (epoch #0): 100%|██████████| 830/830 [00:15<00:00, 55.14it/s, loss=0.299, running_loss=1]    \n",
      "Training (epoch #1): 100%|██████████| 830/830 [00:14<00:00, 57.28it/s, loss=0.366, running_loss=1.01] \n",
      "Training (epoch #2): 100%|██████████| 830/830 [00:14<00:00, 56.88it/s, loss=0.34, running_loss=1.01] \n",
      "Training (epoch #3): 100%|██████████| 830/830 [00:14<00:00, 57.12it/s, loss=0.413, running_loss=1.02] \n",
      "Training (epoch #4): 100%|██████████| 830/830 [00:14<00:00, 57.30it/s, loss=0.516, running_loss=0.999]\n",
      "Training (epoch #0): 100%|██████████| 830/830 [00:14<00:00, 55.36it/s, loss=0.334, running_loss=0.49]  \n",
      "Training (epoch #1): 100%|██████████| 830/830 [00:14<00:00, 56.66it/s, loss=0.416, running_loss=0.26]  \n",
      "Training (epoch #2): 100%|██████████| 830/830 [00:14<00:00, 56.80it/s, loss=0.136, running_loss=0.241] \n",
      "Training (epoch #3): 100%|██████████| 830/830 [00:14<00:00, 56.95it/s, loss=0.0648, running_loss=0.198]\n",
      "Training (epoch #4): 100%|██████████| 830/830 [00:14<00:00, 56.24it/s, loss=0.111, running_loss=0.18]  \n",
      "Training (epoch #0): 100%|██████████| 830/830 [00:14<00:00, 55.59it/s, loss=0.259, running_loss=0.667] \n",
      "Training (epoch #1): 100%|██████████| 830/830 [00:14<00:00, 56.31it/s, loss=0.191, running_loss=0.213] \n",
      "Training (epoch #2): 100%|██████████| 830/830 [00:14<00:00, 56.70it/s, loss=0.093, running_loss=0.156] \n",
      "Training (epoch #3): 100%|██████████| 830/830 [00:14<00:00, 56.63it/s, loss=0.0883, running_loss=0.119]\n",
      "Training (epoch #4): 100%|██████████| 830/830 [00:14<00:00, 56.42it/s, loss=0.0245, running_loss=0.099] \n",
      "Training (epoch #0): 100%|██████████| 830/830 [00:14<00:00, 55.57it/s, loss=0.577, running_loss=1.29] \n",
      "Training (epoch #1): 100%|██████████| 830/830 [00:15<00:00, 55.10it/s, loss=0.585, running_loss=1.24] \n",
      "Training (epoch #2): 100%|██████████| 830/830 [00:14<00:00, 55.49it/s, loss=0.585, running_loss=1.25] \n",
      "Training (epoch #3): 100%|██████████| 830/830 [00:15<00:00, 55.33it/s, loss=0.32, running_loss=1.32] \n",
      "Training (epoch #4): 100%|██████████| 830/830 [00:15<00:00, 55.31it/s, loss=0.328, running_loss=1.23]\n",
      "Training (epoch #0): 100%|██████████| 830/830 [00:15<00:00, 54.58it/s, loss=0.261, running_loss=0.759] \n",
      "Training (epoch #1): 100%|██████████| 830/830 [00:15<00:00, 54.97it/s, loss=0.0929, running_loss=0.463]\n",
      "Training (epoch #2): 100%|██████████| 830/830 [00:15<00:00, 55.03it/s, loss=0.264, running_loss=0.384] \n",
      "Training (epoch #3): 100%|██████████| 830/830 [00:14<00:00, 55.73it/s, loss=0.182, running_loss=0.298] \n",
      "Training (epoch #4): 100%|██████████| 830/830 [00:15<00:00, 54.49it/s, loss=0.219, running_loss=0.267] \n",
      "Training (epoch #0): 100%|██████████| 830/830 [00:15<00:00, 54.36it/s, loss=0.166, running_loss=0.539] \n",
      "Training (epoch #1): 100%|██████████| 830/830 [00:15<00:00, 54.62it/s, loss=0.0878, running_loss=0.168]\n",
      "Training (epoch #2): 100%|██████████| 830/830 [00:15<00:00, 55.22it/s, loss=0.05, running_loss=0.117]  \n",
      "Training (epoch #3): 100%|██████████| 830/830 [00:15<00:00, 54.94it/s, loss=0.0389, running_loss=0.0885] \n",
      "Training (epoch #4): 100%|██████████| 830/830 [00:15<00:00, 55.05it/s, loss=0.0252, running_loss=0.0712] \n",
      "Training (epoch #0): 100%|██████████| 415/415 [00:08<00:00, 47.74it/s, loss=0.744, running_loss=1.04]\n",
      "Training (epoch #1): 100%|██████████| 415/415 [00:08<00:00, 50.42it/s, loss=0.624, running_loss=1.06]\n",
      "Training (epoch #2): 100%|██████████| 415/415 [00:08<00:00, 50.10it/s, loss=0.725, running_loss=1.04]\n",
      "Training (epoch #3): 100%|██████████| 415/415 [00:08<00:00, 50.92it/s, loss=0.803, running_loss=1.02] \n",
      "Training (epoch #4): 100%|██████████| 415/415 [00:08<00:00, 49.95it/s, loss=0.874, running_loss=0.996]\n",
      "Training (epoch #0): 100%|██████████| 415/415 [00:08<00:00, 49.27it/s, loss=0.519, running_loss=1.01]\n",
      "Training (epoch #1): 100%|██████████| 415/415 [00:08<00:00, 49.03it/s, loss=0.528, running_loss=0.645]\n",
      "Training (epoch #2): 100%|██████████| 415/415 [00:08<00:00, 49.56it/s, loss=0.353, running_loss=0.418] \n",
      "Training (epoch #3): 100%|██████████| 415/415 [00:08<00:00, 48.96it/s, loss=0.562, running_loss=0.351] \n",
      "Training (epoch #4): 100%|██████████| 415/415 [00:08<00:00, 50.47it/s, loss=0.403, running_loss=0.354] \n",
      "Training (epoch #0): 100%|██████████| 415/415 [00:08<00:00, 48.79it/s, loss=0.594, running_loss=1.64]\n",
      "Training (epoch #1): 100%|██████████| 415/415 [00:08<00:00, 51.05it/s, loss=0.416, running_loss=0.611]\n",
      "Training (epoch #2): 100%|██████████| 415/415 [00:08<00:00, 49.33it/s, loss=0.37, running_loss=0.317]  \n",
      "Training (epoch #3): 100%|██████████| 415/415 [00:08<00:00, 49.16it/s, loss=0.25, running_loss=0.254]  \n",
      "Training (epoch #4): 100%|██████████| 415/415 [00:08<00:00, 50.41it/s, loss=0.207, running_loss=0.225] \n",
      "Training (epoch #0): 100%|██████████| 415/415 [00:08<00:00, 47.90it/s, loss=0.932, running_loss=1.27]\n",
      "Training (epoch #1): 100%|██████████| 415/415 [00:08<00:00, 48.85it/s, loss=0.633, running_loss=1.12]\n",
      "Training (epoch #2): 100%|██████████| 415/415 [00:08<00:00, 50.92it/s, loss=0.559, running_loss=1.08]\n",
      "Training (epoch #3): 100%|██████████| 415/415 [00:08<00:00, 48.92it/s, loss=0.438, running_loss=1.01]\n",
      "Training (epoch #4): 100%|██████████| 415/415 [00:08<00:00, 49.74it/s, loss=0.499, running_loss=1.01]\n",
      "Training (epoch #0): 100%|██████████| 415/415 [00:08<00:00, 47.20it/s, loss=0.223, running_loss=0.507] \n",
      "Training (epoch #1): 100%|██████████| 415/415 [00:09<00:00, 44.21it/s, loss=0.221, running_loss=0.27]  \n",
      "Training (epoch #2): 100%|██████████| 415/415 [00:09<00:00, 44.28it/s, loss=0.139, running_loss=0.216] \n",
      "Training (epoch #3): 100%|██████████| 415/415 [00:08<00:00, 46.70it/s, loss=0.235, running_loss=0.201] \n",
      "Training (epoch #4): 100%|██████████| 415/415 [00:08<00:00, 48.42it/s, loss=0.198, running_loss=0.182] \n",
      "Training (epoch #0): 100%|██████████| 415/415 [00:08<00:00, 47.03it/s, loss=0.287, running_loss=0.894]\n",
      "Training (epoch #1): 100%|██████████| 415/415 [00:08<00:00, 48.81it/s, loss=0.162, running_loss=0.243] \n",
      "Training (epoch #2): 100%|██████████| 415/415 [00:08<00:00, 49.08it/s, loss=0.138, running_loss=0.166] \n",
      "Training (epoch #3): 100%|██████████| 415/415 [00:08<00:00, 49.83it/s, loss=0.138, running_loss=0.126] \n",
      "Training (epoch #4): 100%|██████████| 415/415 [00:08<00:00, 50.00it/s, loss=0.0947, running_loss=0.105]\n",
      "Training (epoch #0): 100%|██████████| 415/415 [00:08<00:00, 47.87it/s, loss=0.699, running_loss=1.32]\n",
      "Training (epoch #1): 100%|██████████| 415/415 [00:08<00:00, 48.33it/s, loss=0.72, running_loss=1.19] \n",
      "Training (epoch #2): 100%|██████████| 415/415 [00:08<00:00, 49.10it/s, loss=0.715, running_loss=1.19]\n",
      "Training (epoch #3): 100%|██████████| 415/415 [00:08<00:00, 49.01it/s, loss=0.717, running_loss=1.19]\n",
      "Training (epoch #4): 100%|██████████| 415/415 [00:08<00:00, 48.05it/s, loss=0.718, running_loss=1.19]\n",
      "Training (epoch #0): 100%|██████████| 415/415 [00:09<00:00, 45.66it/s, loss=0.141, running_loss=0.476] \n",
      "Training (epoch #1): 100%|██████████| 415/415 [00:08<00:00, 46.36it/s, loss=0.194, running_loss=0.238] \n",
      "Training (epoch #2): 100%|██████████| 415/415 [00:09<00:00, 45.77it/s, loss=0.131, running_loss=0.193] \n",
      "Training (epoch #3): 100%|██████████| 415/415 [00:08<00:00, 47.09it/s, loss=0.159, running_loss=0.162] \n",
      "Training (epoch #4): 100%|██████████| 415/415 [00:08<00:00, 47.04it/s, loss=0.0999, running_loss=0.146]\n",
      "Training (epoch #0): 100%|██████████| 415/415 [00:08<00:00, 47.77it/s, loss=0.189, running_loss=0.678] \n",
      "Training (epoch #1): 100%|██████████| 415/415 [00:08<00:00, 48.02it/s, loss=0.0961, running_loss=0.201]\n",
      "Training (epoch #2): 100%|██████████| 415/415 [00:08<00:00, 49.25it/s, loss=0.0591, running_loss=0.133]\n",
      "Training (epoch #3): 100%|██████████| 415/415 [00:08<00:00, 48.09it/s, loss=0.0615, running_loss=0.0954]\n",
      "Training (epoch #4): 100%|██████████| 415/415 [00:08<00:00, 48.46it/s, loss=0.0971, running_loss=0.0789]\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame(columns=['batch_size', 'hidden_size', 'learning_rate', 'losses_test', 'losses_test_scaf'])\n",
    "\n",
    "for bs in batch_size:\n",
    "    for hs in hidden_size:\n",
    "        for lr in learning_rate:\n",
    "            \n",
    "            model = RNN(len(data.chars) + 1, hs).to(device)\n",
    "\n",
    "            train_loader = DataLoader(train, batch_size=bs, collate_fn = collate)\n",
    "            test_loader = DataLoader(test, batch_size=bs, collate_fn = collate)\n",
    "            test_scaffolds_loader = DataLoader(test_scaffolds, batch_size=bs, collate_fn = collate)\n",
    "\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(get_params(), lr=lr)\n",
    "\n",
    "            model.zero_grad()\n",
    "            for epoch in range(5):\n",
    "                tqdm_data = tqdm(train_loader, desc='Training (epoch #{})'.format(epoch))\n",
    "                postfix = train_epoch(model, tqdm_data, criterion, optimizer)\n",
    "\n",
    "            test_loss = test_model(model, test_loader, criterion)\n",
    "            test_scaffolds_loss = test_model(model, test_scaffolds_loader, criterion)\n",
    "\n",
    "            df_result.loc[len(df_result.index)] = [bs, hs, lr, float(test_loss), float(test_scaffolds_loss)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результаты"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что в оптимизируемых параметрах отсутствует какая-либо тенденция. При этом наилучшим сочетанием оказались параметры batch_size = 20, hidden_size = 100, learning_rate = 0.001, что соответствует ошибке модели равной 0.075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>losses_test</th>\n",
       "      <th>losses_test_scaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.075192</td>\n",
       "      <td>0.069648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.090875</td>\n",
       "      <td>0.091055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.099885</td>\n",
       "      <td>0.093177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.106250</td>\n",
       "      <td>0.114236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.112716</td>\n",
       "      <td>0.101571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  hidden_size  learning_rate  losses_test  losses_test_scaf\n",
       "26        20.0        100.0          0.001     0.075192          0.069648\n",
       "17        10.0        100.0          0.001     0.090875          0.091055\n",
       "23        20.0         50.0          0.001     0.099885          0.093177\n",
       "8          5.0        100.0          0.001     0.106250          0.114236\n",
       "14        10.0         50.0          0.001     0.112716          0.101571"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.sort_values(by=['losses_test']).head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN показала улучшенные результаты в сравнении с сетями на фингерпринтах, минимальная ошибка которых составляла 0.15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
